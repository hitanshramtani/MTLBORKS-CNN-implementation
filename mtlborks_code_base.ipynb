{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea841233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "import random \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from scipy.spatial.distance import euclidean\n",
    "import logging\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a2ddbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 19\n",
    "N = 20\n",
    "\n",
    "T_max =  10\n",
    "\n",
    "NConv_min =  1\n",
    "NConv_max =  3\n",
    "\n",
    "NFil_min =  3\n",
    "NFil_max =  256\n",
    "\n",
    "SKer_min =  3\n",
    "SKer_max =  9\n",
    "\n",
    "SPool_min =  1\n",
    "SPool_max =  3\n",
    "\n",
    "SStr_min =  1\n",
    "SStr_max =  2\n",
    "\n",
    "NFc_min =  1\n",
    "NFc_max =  2\n",
    "\n",
    "NNeu_min =  1\n",
    "NNeu_max =  300\n",
    "\n",
    "S_batch =  128\n",
    "\n",
    "epochs_fitness =  1\n",
    "epochs_full_training  = 100\n",
    "R_L  = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b0a4894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_constraints(offspring_vec,\n",
    "                      NConv_min, NConv_max,\n",
    "                      NFil_min, NFil_max,\n",
    "                      SKer_min, SKer_max,\n",
    "                      SPool_min, SPool_max,\n",
    "                      SStr_min, SStr_max,\n",
    "                      NFc_min, NFc_max,\n",
    "                      NNeu_min, Nneu_max):\n",
    "    logging.info(\"doing boundry check  and rounding integer of new offspring generated in teacher phase\")\n",
    "    D = len(offspring_vec)\n",
    "    \n",
    "    #N_conv\n",
    "    offspring_vec[0] = np.clip(offspring_vec[0], NConv_min, NConv_max)\n",
    "    # Conv layers\n",
    "    for l in range(1, NConv_max + 1):\n",
    "        offspring_vec[2*l - 1] = np.clip(offspring_vec[2*l - 1], NFil_min, NFil_max)\n",
    "        offspring_vec[2*l] = np.clip(offspring_vec[2*l], SKer_min, SKer_max)\n",
    "    #Pool layers\n",
    "    base_idx = 2 * NConv_max\n",
    "    for l in range(1, NConv_max + 1):\n",
    "        offspring_vec[base_idx + 3*l - 2] = np.clip(offspring_vec[base_idx + 3*l - 2], 0, 1) # P_pool\n",
    "        offspring_vec[base_idx + 3*l - 1] = np.clip(offspring_vec[base_idx + 3*l - 1], SPool_min, SPool_max)\n",
    "        offspring_vec[base_idx + 3*l] = np.clip(offspring_vec[base_idx + 3*l], SStr_min, SStr_max)\n",
    "    # N_fc\n",
    "    fc_base_idx = 5 * NConv_max + 1\n",
    "    offspring_vec[fc_base_idx] = np.clip(offspring_vec[fc_base_idx], NFc_min, NFc_max)\n",
    "    # FC layers\n",
    "    for q in range(1, NFc_max + 1):\n",
    "        offspring_vec[fc_base_idx + q] = np.clip(offspring_vec[fc_base_idx + q],NNeu_min, Nneu_max)\n",
    "        \n",
    "    #rounding\n",
    "    offspring_vec[0] = round(offspring_vec[0])\n",
    "    for l in range(1, NConv_max + 1):\n",
    "        offspring_vec[2*l - 1] = round(offspring_vec[2*l - 1])\n",
    "        offspring_vec[2*l] = round(offspring_vec[2*l])\n",
    "    base_idx = 2 * NConv_max\n",
    "    for l in range(1, NConv_max + 1):\n",
    "        #do NOT round the P_pool probability\n",
    "        offspring_vec[base_idx + 3*l - 1] = round(offspring_vec[base_idx + 3*l - 1])\n",
    "        offspring_vec[base_idx + 3*l] = round(offspring_vec[base_idx + 3*l])\n",
    "    fc_base_idx = 5 * NConv_max + 1\n",
    "    offspring_vec[fc_base_idx] = round(offspring_vec[fc_base_idx])\n",
    "    for q in range(1, NFc_max + 1):\n",
    "        offspring_vec[fc_base_idx + q] = round(offspring_vec[fc_base_idx + q])\n",
    "        \n",
    "    return offspring_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5eed39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boundary_arrays(NConv_min, NConv_max,\n",
    "    NFil_min, NFil_max,\n",
    "    SKer_min, SKer_max,\n",
    "    SPool_min, SPool_max,\n",
    "    SStr_min, SStr_max,\n",
    "    NFC_min, NFC_max,\n",
    "    NNeu_min, NNeu_max):\n",
    "    logging.info(\"xmin and xmax initialize\")\n",
    "    D = 5 * NConv_max + NFC_max + 2\n",
    "    X_min = np.zeros(D)\n",
    "    X_max = np.zeros(D)\n",
    "    # N_conv\n",
    "    X_min[0], X_max[0] = NConv_min, NConv_max\n",
    "    # Conv layers\n",
    "    for l in range(1, NConv_max + 1):\n",
    "        X_min[2*l - 1], X_max[2*l - 1] = NFil_min, NFil_max\n",
    "        X_min[2*l], X_max[2*l] = SKer_min, SKer_max\n",
    "    # Pool layers\n",
    "    base_idx = 2 * NConv_max\n",
    "    for l in range(1, NConv_max + 1):\n",
    "        X_min[base_idx + 3*l - 2], X_max[base_idx + 3*l - 2] = 0.0, 1.0\n",
    "        X_min[base_idx + 3*l - 1], X_max[base_idx + 3*l - 1] = SPool_min, SPool_max\n",
    "        X_min[base_idx + 3*l], X_max[base_idx + 3*l] = SStr_min, SStr_max\n",
    "    # N_fc\n",
    "    fc_base_idx = 5 * NConv_max + 1\n",
    "    X_min[fc_base_idx], X_max[fc_base_idx] = NFC_min, NFC_max\n",
    "    # FC layers\n",
    "    for q in range(1, NFC_max + 1):\n",
    "        X_min[fc_base_idx + q], X_max[fc_base_idx + q] = NNeu_min, NNeu_max\n",
    "    logging.info(f\"xmax {X_max}\\nx min  {X_min}\")\n",
    "    return X_min, X_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cbbe52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_learning_schema(learner,teacher,\n",
    "                        X_max, X_min,\n",
    "                        NConv_min, NConv_max,\n",
    "                        NFil_min, NFil_max,\n",
    "                        SKer_min, SKer_max,\n",
    "                        SPool_min, SPool_max,\n",
    "                        SStr_min, SStr_max,\n",
    "                        NFc_min, NFc_max,\n",
    "                        NNeu_min, Nneu_max,\n",
    "                        R_train, R_valid,S_batch = 128, ε_train = 1,\n",
    "                        R_L = 0.001, C_num = 10,\n",
    "                        input_shape =  (28,28,1)\n",
    "                        ):\n",
    "    D = len(learner[\"vector\"])\n",
    "    Xn = learner[\"vector\"].copy()\n",
    "    d_rand = random.randint(0, D - 1)\n",
    "    r5 = random.uniform(-1, 1)\n",
    "    Xn[d_rand] += r5 * (X_max[d_rand] - X_min[d_rand])\n",
    "    Xn = apply_constraints(Xn,\n",
    "                NConv_min, NConv_max,\n",
    "                NFil_min, NFil_max,\n",
    "                SKer_min, SKer_max,\n",
    "                SPool_min, SPool_max,\n",
    "                SStr_min, SStr_max,\n",
    "                NFc_min, NFc_max,\n",
    "                NNeu_min, Nneu_max )\n",
    "    \n",
    "    fitness_self_learner = evaluate_fitness(Xn, R_train, R_valid, NConv_max,S_batch = 128, ε_train = 1,\n",
    "                                    R_L = 0.001, C_num = 10,\n",
    "                                    input_shape =  (28,28,1))\n",
    "    if fitness_self_learner < learner['fitness']:\n",
    "        updated_learner = {\"vector\": Xn, \"fitness\": fitness_self_learner}\n",
    "        if fitness_self_learner < teacher['fitness']:\n",
    "            teacher['vector'] = Xn.copy()\n",
    "            teacher['fitness'] = fitness_self_learner\n",
    "            logging.info(f\"New Teacher found via Self-Learning Fitness: {fitness_self_learner:.4f} ***\")\n",
    "        return updated_learner,teacher\n",
    "    else:\n",
    "        return learner, teacher\n",
    "\n",
    "\n",
    "def adaptive_peer_learning(n, P_off, teacher,NConv_min, NConv_max,\n",
    "                                NFil_min, NFil_max,\n",
    "                                SKer_min, SKer_max,\n",
    "                                SPool_min, SPool_max,\n",
    "                                SStr_min, SStr_max,\n",
    "                                NFc_min, NFc_max,\n",
    "                                NNeu_min, Nneu_max,R_train,R_valid,S_batch = 128, ε_train = 1,\n",
    "                                R_L = 0.001, C_num = 10,\n",
    "                                input_shape =  (28,28,1)):\n",
    "    N = len(P_off)\n",
    "    D = len(P_off[0]['vector'])\n",
    "    learner = P_off[n]\n",
    "    Xn = learner['vector'].copy()\n",
    "    rank = (N - 1) - n\n",
    "    p_n_PL = rank / N\n",
    "    peer_indices = list(range(N))\n",
    "    peer_indices.remove(n)\n",
    "    p, s, u = random.sample(peer_indices, 3)\n",
    "    peer_p, peer_s, peer_u = P_off[p]['vector'], P_off[s]['vector'], P_off[u]['vector']\n",
    "    chi_n = random.uniform(0.5, 1.0)\n",
    "    for d in range(D):\n",
    "        if random.random() < p_n_PL:\n",
    "            Xn[d] = peer_p[d] + chi_n * (peer_s[d] - peer_u[d])\n",
    "    Xn = apply_constraints(Xn,\n",
    "                NConv_min, NConv_max,\n",
    "                NFil_min, NFil_max,\n",
    "                SKer_min, SKer_max,\n",
    "                SPool_min, SPool_max,\n",
    "                SStr_min, SStr_max,\n",
    "                NFc_min, NFc_max,\n",
    "                NNeu_min, Nneu_max )\n",
    "    \n",
    "    fitness_adaptive_learner = evaluate_fitness(Xn, R_train, R_valid, NConv_max,S_batch = 128, ε_train = 1,\n",
    "                                    R_L = 0.001, C_num = 10,\n",
    "                                    input_shape =  (28,28,1))\n",
    "    if fitness_adaptive_learner < learner['fitness']:\n",
    "        updated_learner = {\"vector\": Xn, \"fitness\": fitness_adaptive_learner}\n",
    "        if fitness_adaptive_learner < teacher['fitness']:\n",
    "            teacher['vector'] = Xn.copy()\n",
    "            teacher['fitness'] = fitness_adaptive_learner\n",
    "            logging.info(f\"New Teacher found via Self-Learning Fitness: {fitness_adaptive_learner:.4f} ***\")\n",
    "        return updated_learner,teacher\n",
    "    else:\n",
    "        return learner, teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35483222",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-22 13:34:05,953 - INFO - xmin and xmax initialize\n",
      "2025-08-22 13:34:05,979 - INFO - xmax [  3. 256.   9. 256.   9. 256.   9.   1.   3.   2.   1.   3.   2.   1.\n",
      "   3.   2.   2. 300. 300.]\n",
      "x min  [1. 3. 3. 3. 3. 3. 3. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1.]\n",
      "2025-08-22 13:34:05,981 - INFO - Initializing population\n",
      "2025-08-22 13:34:05,985 - INFO - Evaluating Fitness\n",
      "2025-08-22 13:34:05,986 - INFO - making cnn(decoding xn to cnn model)\n",
      "2025-08-22 13:39:44,080 - INFO - Learner 1 initialized with fitness: 0.2672\n",
      "2025-08-22 13:39:44,124 - INFO - *** New Teacher found! Fitness: 0.2672 ***\n",
      "2025-08-22 13:39:44,126 - INFO - Evaluating Fitness\n",
      "2025-08-22 13:39:44,127 - INFO - making cnn(decoding xn to cnn model)\n",
      "2025-08-22 13:43:18,837 - INFO - Learner 2 initialized with fitness: 0.5895\n",
      "2025-08-22 13:43:18,839 - INFO - Evaluating Fitness\n",
      "2025-08-22 13:43:18,840 - INFO - making cnn(decoding xn to cnn model)\n",
      "2025-08-22 13:46:25,126 - INFO - Learner 3 initialized with fitness: 0.1466\n",
      "2025-08-22 13:46:25,128 - INFO - *** New Teacher found! Fitness: 0.1466 ***\n",
      "2025-08-22 13:46:25,130 - INFO - Evaluating Fitness\n",
      "2025-08-22 13:46:25,131 - INFO - making cnn(decoding xn to cnn model)\n",
      "2025-08-22 13:53:12,566 - INFO - Learner 4 initialized with fitness: 0.4411\n",
      "2025-08-22 13:53:12,568 - INFO - Evaluating Fitness\n",
      "2025-08-22 13:53:12,568 - INFO - making cnn(decoding xn to cnn model)\n",
      "2025-08-22 13:54:55,662 - INFO - Learner 5 initialized with fitness: 0.1611\n",
      "2025-08-22 13:54:55,664 - INFO - Evaluating Fitness\n",
      "2025-08-22 13:54:55,664 - INFO - making cnn(decoding xn to cnn model)\n",
      "2025-08-22 14:02:22,391 - INFO - Learner 6 initialized with fitness: 0.1394\n",
      "2025-08-22 14:02:22,392 - INFO - *** New Teacher found! Fitness: 0.1394 ***\n",
      "2025-08-22 14:02:22,393 - INFO - Evaluating Fitness\n",
      "2025-08-22 14:02:22,394 - INFO - making cnn(decoding xn to cnn model)\n",
      "2025-08-22 14:02:48,660 - INFO - Learner 7 initialized with fitness: 0.1557\n",
      "2025-08-22 14:02:48,663 - INFO - Evaluating Fitness\n",
      "2025-08-22 14:02:48,665 - INFO - making cnn(decoding xn to cnn model)\n",
      "2025-08-22 14:10:57,673 - INFO - Learner 8 initialized with fitness: 0.2050\n",
      "2025-08-22 14:10:57,684 - INFO - Evaluating Fitness\n",
      "2025-08-22 14:10:57,686 - INFO - making cnn(decoding xn to cnn model)\n"
     ]
    }
   ],
   "source": [
    "def initialize_population(\n",
    "    N,  # population size\n",
    "    NConv_min, NConv_max,\n",
    "    NFil_min, NFil_max,\n",
    "    SKer_min, SKer_max,\n",
    "    SPool_min, SPool_max,\n",
    "    SStr_min, SStr_max,\n",
    "    NFC_min, NFC_max,\n",
    "    NNeu_min, NNeu_max,\n",
    "    R_train, R_valid\n",
    "):\n",
    "    logging.info(\"Initializing population\")\n",
    "    D = 5 * NConv_max + NFC_max + 2\n",
    "    teacher_solution = {\n",
    "        \"vector\": None,\n",
    "        \"fitness\": float('inf')\n",
    "    }\n",
    "    population = []\n",
    "    for n in range(N):\n",
    "        Xn = np.zeros(D)\n",
    "        NConv = random.randint(NConv_min, NConv_max) #initializing eith number of learners\n",
    "        Xn[0] = NConv\n",
    "        for l in range(1,NConv_max+1):\n",
    "            NFil = random.randint(NFil_min, NFil_max)\n",
    "            SKer = random.randint(SKer_min, SKer_max)\n",
    "            Xn[2*l - 1] = NFil\n",
    "            Xn[2*l] = SKer\n",
    "        for l in range(1,NConv_max+1):\n",
    "            PPool = random.uniform(0, 1)\n",
    "            SPool = random.randint(SPool_min, SPool_max)\n",
    "            SStr = random.randint(SStr_min, SStr_max)\n",
    "            base_idx = 2 * NConv_max\n",
    "            Xn[base_idx+3*l-2] = PPool\n",
    "            Xn[base_idx+3*l-1] = SPool\n",
    "            Xn[base_idx+3*l] = SStr\n",
    "        fc_base_idx = 5 * NConv_max + 1\n",
    "        Xn[fc_base_idx]= random.randint(NFC_min, NFC_max)\n",
    "        for q in range(1,NFC_max+1):\n",
    "            Xn[fc_base_idx + q]= random.randint(NNeu_min, NNeu_max)\n",
    "        \n",
    "        fitness = evaluate_fitness(Xn,R_train, R_valid,NConv_max)\n",
    "\n",
    "        learner_solution = {\n",
    "            \"vector\": Xn,\n",
    "            \"fitness\": fitness\n",
    "        }\n",
    "        population.append(learner_solution)\n",
    "        logging.info(f\"Learner {n+1} initialized with fitness: {fitness:.4f}\")\n",
    "        if fitness < teacher_solution[\"fitness\"]:\n",
    "            teacher_solution[\"vector\"] = Xn.copy()\n",
    "            teacher_solution[\"fitness\"] = fitness\n",
    "            logging.info(f\"*** New Teacher found! Fitness: {fitness:.4f} ***\")\n",
    "    return population, teacher_solution\n",
    "        \n",
    "        \n",
    "def evaluate_fitness(\n",
    "    Xn,R_train, R_valid,NConv_max,\n",
    "    S_batch = 128, ε_train = 1,\n",
    "    R_L = 0.001, C_num = 10,\n",
    "    input_shape =  (28,28,1)\n",
    "):\n",
    "    logging.info(\"Evaluating Fitness\")\n",
    "    try:\n",
    "        # logging.info(\"trying statement\")\n",
    "        model = decode_learner_to_cnn(\n",
    "                Xn, NConv_max,\n",
    "                input_shape,\n",
    "                C_num\n",
    "            )\n",
    "        # logging.info(1)\n",
    "        optimizer = Adam(learning_rate=R_L)\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        # logging.info(2)\n",
    "        x_train, y_train = R_train\n",
    "        x_val, y_val = R_valid\n",
    "        model.fit(\n",
    "                x_train, y_train,\n",
    "                epochs=ε_train,\n",
    "                batch_size=S_batch,\n",
    "                validation_data=R_valid,\n",
    "                verbose=0  \n",
    "            )\n",
    "        # logging.info(3)\n",
    "        \n",
    "        loss, accuracy = model.evaluate(x_val, y_val, verbose=0)\n",
    "        # The fitness is simply the error\n",
    "        classification_error = 1.0 - accuracy\n",
    "        return classification_error\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.info(f\"Error in fitness evaluation\")\n",
    "        return float('inf')\n",
    "\n",
    "\n",
    "def decode_learner_to_cnn(Xn, NConv_max,\n",
    "                          input_shape =  (28,28,1),C_num = 10\n",
    "            ):\n",
    "    logging.info(\"making cnn(decoding xn to cnn model)\")\n",
    "    model = Sequential()\n",
    "    model.add(tf.keras.Input(shape=input_shape))\n",
    "    NConv = int(Xn[0])\n",
    "    NFC = int(Xn[5 * NConv_max +1])\n",
    "    for l in range(1,NConv+1):\n",
    "        NFil = int(Xn[2 * l-1])\n",
    "        SKer = int(Xn[2 * l])\n",
    "        model.add(Conv2D(\n",
    "                filters=NFil,\n",
    "                kernel_size=(SKer, SKer),\n",
    "                activation='relu',\n",
    "                padding='valid',\n",
    "                kernel_initializer='he_normal' \n",
    "            ))\n",
    "        model.add(BatchNormalization())\n",
    "        pooling_type_d = 2 * NConv_max + 3 * l - 2\n",
    "        kernal_size_of_pooling_layer = 2 * NConv_max + 3 * l - 1\n",
    "        stride_size_of_polling_layer = 2 * NConv_max + 3 * l \n",
    "        PPool = Xn[pooling_type_d]\n",
    "        SPool = int(Xn[kernal_size_of_pooling_layer])\n",
    "        SStr = int(Xn[stride_size_of_polling_layer])\n",
    "        if 0 <= PPool < 1/3:\n",
    "            pass  # No pooling\n",
    "        elif 1/3 <= PPool < 2/3:\n",
    "            model.add(MaxPooling2D(pool_size=(SPool, SPool), strides=(SStr, SStr)))\n",
    "        else:  # PPool ≥ 2/3\n",
    "            model.add(AveragePooling2D(pool_size=(SPool, SPool), strides=(SStr, SStr)))\n",
    "        \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    for q in range(1, NFC + 1):\n",
    "        dimension_of_fully_connected_layer = (5 * NConv_max + 1) + q\n",
    "        num_neurons = int(Xn[dimension_of_fully_connected_layer])\n",
    "        model.add(Dense(num_neurons, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "    model.add(Dense(C_num, activation='softmax'))\n",
    "    return model\n",
    "        \n",
    "        \n",
    "def modified_teacher_phase(population, teacher, R_train, R_valid,\n",
    "                           S_batch , ε_train, R_L, C_num,\n",
    "                           NConv_max,NConv_min,\n",
    "                           NFil_min, NFil_max,\n",
    "                           SKer_min, SKer_max,\n",
    "                           SPool_min, SPool_max,\n",
    "                           SStr_min, SStr_max,\n",
    "                           NFc_min, NFc_max,\n",
    "                           NNeu_min, Nneu_max \n",
    "                           ):\n",
    "    # population => [learner_solution = {\n",
    "        #     \"vector\": Xn,\n",
    "        #     \"fitness\": fitness\n",
    "        # }]  -- at ith is knows as learner\n",
    "    # teacher_solution => teacher_solution = {\n",
    "        # \"vector\": None,             #best vector(Xn)\n",
    "    #     \"fitness\": float('inf')     # best fitness\n",
    "    # }\n",
    "    logging.info(\"Starting modified teacher phase\")\n",
    "    sorted_population = sorted(population, key=lambda x: x['fitness'], reverse=True)\n",
    "    N = len(sorted_population)\n",
    "    D = len(sorted_population[0]['vector'])\n",
    "    unique_means = []\n",
    "    for n in range(N):\n",
    "        fitter_learners = sorted_population[n:]\n",
    "        mean_vector = np.mean([learner['vector'] for learner in fitter_learners], axis=0)\n",
    "        unique_means.append(mean_vector)\n",
    "    \n",
    "    unique_social_exemplars = []\n",
    "    for n in range(N):\n",
    "        if n == N - 1:\n",
    "            unique_social_exemplars.append(None)\n",
    "            continue\n",
    "        exemplar_vector = np.zeros(D)\n",
    "        for d in range(D):\n",
    "            better_learner_idx = random.randint(n + 1, N - 1)\n",
    "            exemplar_vector[d] = sorted_population[better_learner_idx]['vector'][d]\n",
    "        unique_social_exemplars.append(exemplar_vector)\n",
    "    \n",
    "    logging.info(\"done making unique and social exemplar\")\n",
    "    \n",
    "    P_off = []\n",
    "    current_teacher = teacher.copy()\n",
    "    for n in range(N):\n",
    "        learner = sorted_population[n]\n",
    "        if n == N - 1:\n",
    "            P_off.append(learner)\n",
    "            logging.info(f\"Best learner {n+1} carried over. Fitness: {learner['fitness']:.4f}\")\n",
    "            continue\n",
    "        \n",
    "        logging.info(f\"Processing Learner {n+1}/{N}...\")\n",
    "        r3, r4 = random.random(), random.random()\n",
    "        F_T = random.choice([1, 2])\n",
    "        teacher_vec = current_teacher['vector']\n",
    "        learner_vec = learner['vector']\n",
    "        mean_vec = unique_means[n]\n",
    "        social_exemplar_vec = unique_social_exemplars[n]\n",
    "        # Xn_Off = learner_vec + r3*(teacher_vec - F_T * mean_vec) + r4*(social_exemplar_vec - learner_vec)\n",
    "        offspring_vector = (learner_vec + \n",
    "                            r3 * (teacher_vec - F_T * mean_vec) + \n",
    "                            r4 * (social_exemplar_vec - learner_vec))\n",
    "        \n",
    "    \n",
    "        \"\"\"Applying Constraints = >\n",
    "            core formula in the Teacher Phase:\n",
    "            offspring_vector = (learner_vec +\n",
    "                                r3 * (teacher_vec - F_T * mean_vec) +\n",
    "                                r4 * (social_exemplar_vec - learner_vec))\n",
    "            This is just vector math. The result of these additions and subtractions is guaranteed to produce numbers that are not clean\n",
    "            simple analogy: Imagine you are designing a car, and the number of doors must be an integer between 2 and 5. You have two designs, one with 2 doors and one with 5. If you average them, you get 3.5 doors. This is a mathematically correct, but you can't build a car with 3.5 doors.\n",
    "            The apply_constraints function is the engineer who looks at the \"3.5 doors\" calculation and says, That's not valid. I'll round that to 4 doors\n",
    "        \"\"\"\n",
    "        offspring_vector = apply_constraints(offspring_vector,\n",
    "                      NConv_min, NConv_max,\n",
    "                      NFil_min, NFil_max,\n",
    "                      SKer_min, SKer_max,\n",
    "                      SPool_min, SPool_max,\n",
    "                      SStr_min, SStr_max,\n",
    "                      NFc_min, NFc_max,\n",
    "                      NNeu_min, Nneu_max \n",
    "                      )\n",
    "        offspring_fitness = evaluate_fitness(offspring_vector, R_train, R_valid, NConv_max, S_batch, ε_train, R_L, C_num)\n",
    "        offspring = {\"vector\": offspring_vector, \"fitness\": offspring_fitness}\n",
    "        P_off.append(offspring)\n",
    "        logging.info(f\"\"\"\n",
    "              before implementing teacher phase learner fitness :{learner['fitness']}\n",
    "              Offspring created with fitness: {offspring_fitness:.4f}\"\"\")\n",
    "        if offspring_fitness < current_teacher['fitness']:\n",
    "            current_teacher['vector'] = offspring_vector.copy()\n",
    "            current_teacher['fitness'] = offspring_fitness\n",
    "            logging.info(f\"  -> *** New Teacher found! Fitness: {offspring_fitness:.4f} ***\")\n",
    "    \n",
    "    return P_off, current_teacher\n",
    "\n",
    "\n",
    "def modified_learner_phase(P_off,teacher,R_train, R_valid,\n",
    "                            X_max, X_min,\n",
    "                            NConv_min, NConv_max,\n",
    "                            NFil_min, NFil_max,\n",
    "                            SKer_min, SKer_max,\n",
    "                            SPool_min, SPool_max,\n",
    "                            SStr_min, SStr_max,\n",
    "                            NFc_min, NFc_max,\n",
    "                            NNeu_min, Nneu_max,\n",
    "                            S_batch = 128, ε_train = 1,\n",
    "                            R_L = 0.001, C_num = 10,\n",
    "                            input_shape =  (28,28,1)):\n",
    "    logging.info(\"Modified_learner_pahse\")\n",
    "    P_off = sorted(P_off,key = lambda x:x[\"fitness\"], reverse = True)\n",
    "    N = len(P_off)\n",
    "    D = len(P_off[0]['vector'])\n",
    "    current_teacher = teacher.copy()\n",
    "    updated_offspring_population = []\n",
    "    for n in range(N):\n",
    "        learner = P_off[n]\n",
    "        logging.info(f\"Processing Learner {n+1}/{N}\")\n",
    "        r6 = random.random()\n",
    "        P_SL = 1/D\n",
    "        if r6<P_SL:\n",
    "            logging.info(\"Performing Self-Learning\")\n",
    "            updated_learner, current_teacher = self_learning_schema(learner,current_teacher,\n",
    "                        X_max, X_min,\n",
    "                        NConv_min, NConv_max,\n",
    "                        NFil_min, NFil_max,\n",
    "                        SKer_min, SKer_max,\n",
    "                        SPool_min, SPool_max,\n",
    "                        SStr_min, SStr_max,\n",
    "                        NFc_min, NFc_max,\n",
    "                        NNeu_min, Nneu_max,\n",
    "                        R_train, R_valid,S_batch, ε_train = 1,\n",
    "                        R_L = 0.001, C_num = 10,\n",
    "                        input_shape =  (28,28,1)\n",
    "                        )\n",
    "        else:\n",
    "            logging.info(\"Performing Adaptive lerning\")\n",
    "            updated_learner, current_teacher = adaptive_peer_learning(n, P_off, current_teacher,NFil_min, NFil_max,\n",
    "                                                    SKer_min, SKer_max,\n",
    "                                                    SPool_min, SPool_max,\n",
    "                                                    SStr_min, SStr_max,\n",
    "                                                    NFc_min, NFc_max,\n",
    "                                                    NNeu_min, Nneu_max,R_train,R_valid,S_batch = 128, ε_train = 1,\n",
    "                                                    R_L = 0.001, C_num = 10,\n",
    "                                                    input_shape =  (28,28,1))\n",
    "        updated_offspring_population.append(updated_learner)\n",
    "    return updated_offspring_population, current_teacher\n",
    "\n",
    "\n",
    "def dual_criterion(N, current_pop, offspring_pop):\n",
    "    logging.info(\"dual_criterion\")\n",
    "    merged_population = current_pop + offspring_pop\n",
    "    merged_population.sort(key=lambda x: x['fitness'])\n",
    "    \n",
    "    best_vector = merged_population[0]['vector']\n",
    "    for learner in merged_population:\n",
    "        learner['diversity'] = euclidean(learner['vector'], best_vector)\n",
    "    K = random.randint(1, N)\n",
    "    next_generation = merged_population[:K]\n",
    "    candidates = merged_population[K:]\n",
    "    if not candidates:\n",
    "        return next_generation\n",
    "    fitness_values = [c['fitness'] for c in candidates]\n",
    "    diversity_values = [c['diversity'] for c in candidates]\n",
    "    F_min, F_max = min(fitness_values), max(fitness_values)\n",
    "    Dis_min, Dis_max = min(diversity_values), max(diversity_values)\n",
    "    for _ in range(N - K):\n",
    "        tourn_a, tourn_b = random.sample(candidates, 2)\n",
    "        alpha = np.random.normal(0.9, 0.05)\n",
    "        alpha = np.clip(alpha, 0.8, 1.0)\n",
    "        f_range = F_max - F_min if F_max > F_min else 1\n",
    "        d_range = Dis_max - Dis_min if Dis_max > Dis_min else 1\n",
    "        wf_a = alpha * ((tourn_a['fitness'] - F_min) / f_range) + (1 - alpha) * ((Dis_max - tourn_a['diversity']) / d_range)\n",
    "        wf_b = alpha * ((tourn_b['fitness'] - F_min) / f_range) + (1 - alpha) * ((Dis_max - tourn_b['diversity']) / d_range)\n",
    "        winner = tourn_a if wf_a <= wf_b else tourn_b\n",
    "        next_generation.append(winner)\n",
    "        candidates.remove(winner)\n",
    "    return next_generation\n",
    "    \n",
    "\n",
    "def load_dataset(dataset_name='mnist'):\n",
    "    if dataset_name.lower() == 'mnist':\n",
    "        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "    elif dataset_name.lower() == 'fashion_mnist':\n",
    "        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "    else:\n",
    "        raise ValueError(f\"Dataset '{dataset_name}' is not supported by this simple loader.\")\n",
    "\n",
    "    x_train = x_train.astype(\"float32\") / 255\n",
    "    x_test = x_test.astype(\"float32\") / 255\n",
    "    \n",
    "    x_train = np.expand_dims(x_train, -1)\n",
    "    x_test = np.expand_dims(x_test, -1)\n",
    "    num_classes = 10\n",
    "    y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "    \n",
    "    R_train = (x_train, y_train)\n",
    "    R_valid = (x_test, y_test)\n",
    "    return R_train, R_valid, x_train.shape[1:], num_classes\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    log_dir = \"loging\"\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    log_file = os.path.join(log_dir, 'result.log')\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file, mode='w'), \n",
    "            logging.StreamHandler() \n",
    "        ]\n",
    "    )\n",
    "    ε_train = epochs_fitness\n",
    "    R_train, R_valid, input_shape, C_num = load_dataset('fashion_mnist')\n",
    "    X_min, X_max = get_boundary_arrays(NConv_min, NConv_max,\n",
    "    NFil_min, NFil_max,\n",
    "    SKer_min, SKer_max,\n",
    "    SPool_min, SPool_max,\n",
    "    SStr_min, SStr_max,\n",
    "    NFc_min, NFc_max,\n",
    "    NNeu_min, NNeu_max)\n",
    "    current_population, teacher = initialize_population(\n",
    "        N,  # population size\n",
    "        NConv_min, NConv_max,\n",
    "        NFil_min, NFil_max,\n",
    "        SKer_min, SKer_max,\n",
    "        SPool_min, SPool_max,\n",
    "        SStr_min, SStr_max,\n",
    "        NFc_min, NFc_max,\n",
    "        NNeu_min, NNeu_max,\n",
    "        R_train, R_valid\n",
    "        )\n",
    "    for t in range(T_max):\n",
    "        logging.info(f\"\\nStarting Generation {t+1}/{T_max}\")\n",
    "        offspring_after_teacher, teacher = modified_teacher_phase(current_population, teacher, R_train, R_valid,\n",
    "                           S_batch , ε_train, R_L, C_num,\n",
    "                           NConv_max,NConv_min,\n",
    "                           NFil_min, NFil_max,\n",
    "                           SKer_min, SKer_max,\n",
    "                           SPool_min, SPool_max,\n",
    "                           SStr_min, SStr_max,\n",
    "                           NFc_min, NFc_max,\n",
    "                           NNeu_min, NNeu_max\n",
    "        )\n",
    "        offspring_after_learner, teacher = modified_learner_phase(\n",
    "            offspring_after_teacher,teacher,R_train, R_valid,\n",
    "            X_max, X_min,\n",
    "            NConv_min, NConv_max,\n",
    "            NFil_min, NFil_max,\n",
    "            SKer_min, SKer_max,\n",
    "            SPool_min, SPool_max,\n",
    "            SStr_min, SStr_max,\n",
    "            NFc_min, NFc_max,\n",
    "            NNeu_min, NNeu_max,\n",
    "            S_batch, ε_train,\n",
    "            R_L , C_num,\n",
    "            input_shape \n",
    "        )\n",
    "        current_population = dual_criterion(N, current_population, offspring_after_learner)\n",
    "        current_population.sort(key=lambda x: x['fitness'])\n",
    "        if current_population[0]['fitness'] < teacher['fitness']:\n",
    "            teacher = current_population[0].copy()\n",
    "            logging.info(f\"Teacher updated after selection. New best fitness: {teacher['fitness']:.4f}\")\n",
    "    \n",
    "    logging.info(\"\\n MTLBORKS-CNN Search Complete\")\n",
    "    logging.info(f\"Final best fitness found (Teacher): {teacher['fitness']:.4f}\")\n",
    "    logging.info(\"Starting full training on the best discovered architecture\")\n",
    "    final_model = decode_learner_to_cnn(teacher['vector'], NConv_max, input_shape,C_num)\n",
    "    final_model.compile(optimizer=Adam(learning_rate=R_L), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    final_model.fit(\n",
    "        R_train[0], R_train[1],\n",
    "        batch_size=S_batch,\n",
    "        epochs=epochs_full_training,\n",
    "        validation_data=R_valid,\n",
    "        verbose=1 \n",
    "    )\n",
    "    \n",
    "    final_loss, final_accuracy = final_model.evaluate(R_valid[0], R_valid[1], verbose=0)\n",
    "    logging.info(\"\\n--- Final Model Performance ---\")\n",
    "    logging.info(f\"Final Accuracy: {final_accuracy * 100:.2f}%\")\n",
    "    logging.info(f\"Final Classification Error: {1 - final_accuracy:.4f}\")\n",
    "    final_model.summary()\n",
    "\n",
    "                \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "                                                   \n",
    "            \n",
    "            \n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c143c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mtl (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
